---
created: 2025-11-30
tags:
  - 笔记
---

# 机器学习全貌与基础

**定义**：机器学习是一门研究算法的学科，它通过**经验 (Experience)** 来提升在某类**任务 (Task)** 上的**性能 (Performance)** 。


| **学习类型**                    | **关键特征**                      | **典型任务**                                     | **PPT案例**  |
| --------------------------- | ----------------------------- | -------------------------------------------- | ---------- |
| **有监督学习** (Supervised)      | **训练数据有标签 (y)**               | **分类** (输出离散标签，如猫/狗) <br>**回归** (输出连续数值，如房价) | 预测房屋价格     |
| **无监督学习** (Unsupervised)    | **训练数据无标签**，只有输入              | **聚类** (自动发现潜在结构) <br>**降维** (如PCA)          | 基因数据降维     |
| **半监督学习** (Semi-supervised) | **少量有标签 + 大量无标签**             | 利用大量未标记数据辅助少量标记数据进行分类                        | —          |
| **强化学习** (Reinforcement)    | **无正确答案**，通过环境给出的**奖励/惩罚**来学习 | 决策控制、游戏AI                                    | 微信小游戏“跳一跳” |

一个标准的机器学习过程包含三个要素 ：
1. **模型 (Model)**：你要建立什么样的映射关系（如分类、回归）。
2. **策略 (Strategy)**：你用什么标准来评估模型好坏（即**损失函数**的选择）。
3. **算法 (Algorithm)**：你怎么求解这个模型的最优参数（如梯度下降、最小二乘法）。

几个实际工程中会遇到的难题：
- **数据稀疏性**：数据量不够，或者有效标注太少 。
- **冷启动 (Cold Start)**：新产品初期没有用户数据，无法推荐 。
- **泛化能力**：训练时表现很好，但遇到没见过的测试数据就“歇菜”了（过拟合） 。
- **可扩展性 (Scalability)**：数据量极大时（如100亿网页），算法跑得动吗？

# 数据工程与准备 (Data Pipeline)

核心逻辑：**数据进来 -> 清洗 -> 拆分 -> 特征处理 -> 调参**。

1. **数据集拆分**
数据划分为 **3份** ：
- **训练集 (Train)**：用来构建模型，“喂”给模型吃。
- **验证集 (Validation)**：这是关键！用来**调整模型参数**（尤其是超参数），并在构建过程中评估模型，提供无偏估计 。
- **测试集 (Test)**：用来**最终评估**模型效果，模拟真实环境 。
**拆分方法**：
- **留出法 (Hold-out)**：简单按比例（如7:3）切分 。
- **K-折交叉验证 (K-Fold)**：将数据分K份，轮流做验证集。通常 **K=10** 。优点是数据利用率高，适合数据量不大的情况。

2. **解决“数据不平衡” (Imbalance)**
如果正样本有99个，负样本只有1个，模型只要全部猜“正”就能有99%准确率，但这没有意义 。 **解决方法** ：
- **过采样 (Over-Sampling)**：复制**少数类**（那个只有1个的负样本），让它变多 。
- **欠采样 (Under-Sampling)**：删掉**多数类**（那些99个正样本），让它变少 。

3. **特征工程 (Feature Engineering)**

- **特征选择**：不是所有特征都有用。
    - **过滤法 (Filter)**：先评分再选，如用**互信息** 。
    - **包裹法 (Wrapper)**：用特定算法去试 。
    - **嵌入法 (Embedded)**：在算法训练过程中自动选，比如 **Lasso回归** (L1正则化) 可以把系数压缩为0，从而通过舍弃特征来实现选择 。
- **特征降维**：矩阵太大算不动怎么办？
    - **PCA (主成分分析)**：无监督，映射到正交空间 。
    - **LDA (线性判别分析)**：有监督，投影后让同类数据更紧密，不同类分得更开 。
- **特征编码**：计算机不认识“男/女”这种字符串。
    - **One-hot 编码**：男=[1,0], 女=[0,1] 。缺点是无法体现语义关联。
    - **词嵌入 (Word Embedding)**：如 Word2Vec，能体现语义（King - Man + Woman ≈ Queen） 。
- **规范化 (Normalization)**：防止大数值特征（如房价）吃掉小数值特征（如房间数）的影响.
    - **标准化**：(x - 均值) / 方差 。
    - **归一化**：(x - min) / (max - min)，缩放到0-1之间 。

4. **超参数调优 (Hyperparameter Tuning)**
- **网格搜索 (Grid Search)**：地毯式搜索，把参数空间划格子，一个一个试。慢，但能找到划定范围内的最优 。
- **随机搜索 (Random Search)**：随机取样。通常比网格搜索快，但不保证是最优 。

# 核心算法——分类 (Classification)

#### **1. 决策树 (Decision Tree)**
- **核心思想**：像人类做决策一样，通过一系列“是/否”的问题（特征判断）将数据分类。
- **怎么选特征？（三种算法对比，必考）：**
    - **ID3**：使用**信息增益**（Info Gain）。倾向于选择取值较多的特征（比如“身份证号”），这是它的缺点。
    - **C4.5**：使用**信息增益比**（Gain Ratio）。引入了惩罚项，修正了 ID3 偏向取值多特征的问题。
    - **CART**：使用**基尼指数**（Gini Index）。基尼指数越小，纯度越高。CART 生成的是**二叉树**。
- **怎么防止过拟合？（剪枝）：**
    - **预剪枝**：在树生长过程中，如果某些指标（如高度、实例个数）达到阈值就提前停止。
    - **后剪枝**：先让树长满，再自底向上把不重要的子树剪掉换成叶子节点。通常比预剪枝效果好，但计算量大。
#### **2. 贝叶斯分类 (Naive Bayes)**
- 核心公式：基于贝叶斯定理。
    $$P(类别|特征) = \frac{P(特征|类别) \times P(类别)}{P(特征)}$$
- **“朴素”在哪里？** 假设所有特征之间是**相互独立**的（比如“是否下雨”和“是否堵车”没关系，虽然现实中往往有关系）。
- **拉普拉斯修正 (Laplace Smoothing)**：
    - **问题**：如果某个特征在训练集中没出现过（概率为 0），连乘会导致整个概率变成 0（一票否决）。
    - **解决**：分子加 1，分母加 N（类别数），避免零概率问题。

#### **3. 支持向量机 (SVM)**
- **目标**：寻找一个超平面，使得两类样本之间的**间隔 (Margin)** 最大化 。
- **支持向量**：决定分类边界的只有距离超平面最近的那几个点，这些点叫“支持向量”。
- **核函数 (Kernel Trick)**：
    - 如果数据在低维空间分不开（线性不可分），就用核函数把它映射到**高维空间**，让它变得线性可分。
    - 常见的核函数：线性核、多项式核、**高斯核 (RBF)**。

#### **4. 逻辑回归 (Logistic Regression)**
- 虽然叫“回归”，但它主要用于**二分类**问题。
- **核心函数**：**Sigmoid 函数**。它能将任意实数映射到 **0 到 1** 之间，正好可以当作概率来看。
    - 预测值 > 0.5 $\rightarrow$ 正类
    - 预测值 < 0.5 $\rightarrow$ 负类
- **损失函数**：对数损失（Log loss）或者叫交叉熵损失，是凸函数，适合用梯度下降法求解。

#### **5. 集成学习 (Ensemble Learning)**

- **Bagging (代表：随机森林)**：
    - **并行**生成多个分类器。
    - 每个分类器的数据是**随机采样**（有放回）的。
    - 最后通过**投票**决定结果。
- **Boosting (代表：AdaBoost)**：
    - **串行**生成分类器。
    - 后一个分类器重点关注前一个分类器**分错的样本**（给分错的样本更高权重） 18181818。


# 核心算法——回归、聚类与其他

## 1. 回归问题 (Regression)

- **线性回归**：
    
    - **核心思想**：试图学到一个线性模型 $f(x) = wx + b$ 来尽可能准确地预测实值输出标记。
        
    - **求解**：通常使用**最小二乘法**或**梯度下降**来最小化均方误差（MSE）。
        
- **正则化回归（重点复习）**：
    
    - **岭回归 (Ridge Regression)**：加 **L2 正则化**（权重的平方和）。它能把系数压缩得很小，但不会变成0。主要用于解决**过拟合**。
        
    - **Lasso 回归**：加 **L1 正则化**（权重的绝对值和）。它能把系数压缩成 **0**，因此具有**特征选择**的功能，适合稀疏数据。
        

## 2. 聚类问题 (Clustering)

聚类是无监督学习，没有标签。

| **算法**           | **核心原理**                           | **关键特点/优缺点**                                                       |
| ---------------- | ---------------------------------- | ------------------------------------------------------------------ |
| **K-means**      | **距离**。选K个中心 -> 分配点 -> 算新中心 -> 迭代。 | **优点**：简单、快。<br>**缺点**：**K值难选**；对**噪声/异常点**敏感；容易陷入**局部最优**；只能分球状簇。 |
| **GMM (高斯混合)**   | **概率**。认为数据是由几个高斯分布混合生成的。          | **软聚类**（Soft Clustering）：输出的是属于某类的**概率**（如70%属于A），而K-means是硬聚类。    |
| **DBSCAN**       | **密度**。基于密度的聚类，找“核心对象”并扩展。         | **优点**：**不需要指定簇个数K**；能发现**任意形状**的簇；对**噪声**不敏感。                     |
| **层次聚类 (AGNES)** | **树状结构**。自底向上，两两合并。                | 不需要预先指定K，可以得到树状的聚类谱系图。                                             |

# 模型评估与优化 (Evaluation & Optimization)

#### **1. 分类模型的评价指标 (必考公式)**

单纯看“准确率”往往是不够的，特别是数据不平衡的时候（比如预测地震，全部猜“不地震”准确率很高，但没用）。

- **混淆矩阵 (Confusion Matrix)**：
    - **TP** (真阳性)、**TN** (真阴性)、**FP** (假阳性/误报)、**FN** (假阴性/漏报)。
- **核心指标**：
    - **准确率 (Accuracy)**：$(TP+TN) / 总数$。整体猜对的比例。
    - **精确率 (Precision)**：$TP / (TP+FP)$。查准率。预测为正的样本中，有多少是真的正？（比如：预测是垃圾邮件的，有多少真的是垃圾邮件？避免误删重要邮件）。
    - **召回率 (Recall)**：$TP / (TP+FN)$。查全率。真实的正样本中，你找出来了多少？（比如：所有地震中，你预测到了几次？宁可错杀一千，不可放过一个） 。
    - **F1-Score**：精确率和召回率的调和平均数，综合衡量指标。
- **曲线指标**：
    - **ROC 曲线**：横轴 FPR，纵轴 TPR。
    - **AUC (Area Under Curve)**：ROC 曲线下的面积。AUC 越大（越接近 1），模型越好。即使样本不平衡，AUC 也能客观评价。

#### **2. 回归与聚类的指标**
- **回归**：
    - **MSE (均方误差)**：预测值与真实值差值的平方和的均值。对大误差非常敏感（平方放大了误差）。
    - **MAE (平均绝对误差)**：差值的绝对值均值。
- **聚类**：
    - **外部指标**：如果有参考答案（标签），比较聚类结果与参考答案的一致性（如 Jaccard 系数）。
    - **内部指标**（无标签时用）：追求**类内距离小，类间距离大**。常用 **DB 指数** (越小越好) 或 **轮廓系数** (Silhouette, 越大越好) 。










