---
created: 2025-12-01
tags:
  - 笔记
---

# 第一部分 神经网络的“细胞”与基石

#### 1. 人工神经元模型 (M-P 模型)

深度学习模仿的是人脑的视觉机理，即从原始信号到低级抽象，再到高级抽象的迭代过程。

- **模型结构**：一个神经元接收来自其他神经元的输入 ($x_i$)，每个输入都有一个对应的**权重** ($w_i$)。
    
- 计算公式：
    
    $$y = f(\sum_{i=1}^{n} w_i x_i - \theta)$$
    
    这里 $\theta$ 是**阈值 (Threshold)**，$f(\cdot)$ 是**激活函数**。
    
- **直观理解**：神经元将输入加权求和，减去阈值，然后通过激活函数决定是否“兴奋”（输出）。
    

#### 2. 激活函数 (Activation Functions)

激活函数引入了**非线性**因素，使得神经网络能拟合复杂的函数。课件重点提到了两种：

- **Sigmoid 函数**：
    
    - 公式：$sigmoid(x) = \frac{1}{1+e^{-x}}$
        
    - 特点：将输出压缩到 (0, 1) 之间。
        
    - _缺点_（后续会讲）：容易导致梯度消失。
        
- **阶跃函数 (Step Function)**：$sgn(x)$，输出为 0 或 1，不可导，现在很少用。
    

#### 3. 感知机 (Perceptron) 与 XOR 问题

- **感知机能力**：罗森布拉特提出的感知机可以实现简单的逻辑运算，如**与 (AND)、或 (OR)、非 (NOT)**。
    
    - _原理_：通过调整权重 $w$ 和阈值 $\theta$，可以画出一条直线将两类模式分开（线性可分）。
        
- **致命缺陷 (XOR 问题)**：感知机无法解决**异或 (XOR)** 问题。
    
    - _原因_：异或问题是非线性可分的（在二维平面上找不到一条直线能把 (0,0)和(1,1) 分到一类，把 (0,1)和(1,0) 分到另一类）。
        
- **解决方案**：增加层数，引入**隐层 (Hidden Layer)**，构建多层前馈网络。
    

#### 4. 误差反向传播算法 (BP 算法)

这是多层网络训练的核心。

- **目标**：最小化损失函数（通常是均方误差 $E$）。
    
- **核心思想**：**梯度下降 (Gradient Descent)**。沿着误差梯度的负方向调整参数。
    
- **链式法则 (Chain Rule)**：BP 算法的数学本质。
    - 因为网络是层层嵌套的函数，计算第一层参数的梯度时，需要利用链式法则，将误差从输出层一层层向回传。
    - **核心公式**（不需死记，但要懂逻辑）：$\Delta w \propto -\eta \frac{\partial E}{\partial w}$，其中 $\eta$ 是学习率。


# 第二部分：深度学习的训练与优化

#### 1. 核心拦路虎：梯度消失 (Gradient Vanishing)

- **现象**：在深层网络中，误差反向传播回第一层时，梯度变得几乎为 0，导致前面的层根本没法更新参数，网络“死”了。
    
- **数学原因 (考点)**：
    
    - BP 算法基于**链式法则**（连乘。
        
    - 旧的激活函数 **Sigmoid** 的导数最大只有 **0.25**。
        
    - 多个小于 1 的数连乘（如 $0.25 \times 0.25 \times \dots$），结果会指数级衰减趋向于 0。
        

#### 2. 三大解决方案 (重点记忆)

为了解决梯度消失，深度学习引入了“三剑客”：

1. **改进激活函数：ReLU (修正线性单元)**
    
    - **公式**：$f(x) = \max(0, x)$ 。
        
    - **优势**：正区间的导数恒为 **1**。不管网络多深，只要激活了，梯度就能原样传回去，不会衰减。
        
    - _变体_：Leaky ReLU (解决负区间梯度为0的问题) 。
        
2. **批归一化 (Batch Normalization, BN)**
    
    - **原理**：在每一层的激活函数之前，强行把神经元的输入拉回到均值为 0、方差为 1 的标准正态分布。
        
    - **作用**：让输入值落在激活函数（哪怕是Sigmoid）的敏感区（非饱和区），避免梯度变小，同时允许使用更大的学习率。
        
3. **残差结构 (ResNet 的核心)**
    
    - **捷径连接 (Shortcut Connection)**：在层与层之间加一条“直连通路”。
        
    - **公式**：$y = F(x) + x$ 。
        
    - **原理**：求导时，$x$ 的导数是 1。这相当于给梯度开了一条“高速公路”，梯度可以无损地流向前面的层，不用完全经过复杂的非线性变换 $F(x)$ 。
        

#### 3. 另一拦路虎：过拟合 (Overfitting)

模型在训练集表现太好，泛化能力差。解决方法：

- **数据增广 (Data Augmentation)**：对图片进行旋转、裁剪、变色，人为制造更多数据。
    
- **提前停止 (Early Stopping)**：训练时监控验证集误差，一旦验证集误差不再下降（反而上升），就立刻停止训练。
    
- **Dropout**：训练时随机“关掉”一部分神经元（让它们不工作），迫使网络不依赖特定的神经元，增强鲁棒性。
    
- **正则化 (L1/L2)**：在损失函数里加一个惩罚项（如 $\lambda ||w||^2$），强迫权重 $w$ 尽可能小，让模型更简单平滑。
    

#### 4. 优化算法 (让梯度下降得更好)

- **SGD (随机梯度下降)**：每次只用这一个（或一批）样本算梯度，虽然有噪声，但能跳出局部极小值。
    
- **Momentum (动量)**：模拟物理动量，更新时加上“上一次的更新量”。像滚石下山，遇到小坑（局部极小）能冲过去。
    
- **Adam**：目前最常用的优化器。结合了 Momentum（动量）和 RMSProp（自适应学习率）的优点，收敛快且稳。


# 第三部分：卷积神经网络 (CNN) 的核心机制

#### 1. 为什么要用 CNN？

- **全连接的痛点**：对于 $1000 \times 1000$ 的图像，如果隐层也有 1M 个神经元，全连接需要 $10^{12}$ 个参数，内存和计算量都无法承受。
    
- **CNN 的三大法宝 (核心考点)：
    
    1. **局部连接 (Local Connectivity)**：神经元不看整张图，只看一个小窗口（感受野），利用了图像的空间相关性 。
        
    2. **权重共享 (Weight Sharing)**：同一个卷积核（比如检测边缘的核）在图像所有位置都用同一组参数 $(w, b)$。这极大地减少了参数量。
        
    3. **下采样 (Subsampling/Pooling)**：降低特征图分辨率，不仅减少计算量，还让特征具有**平移不变性**（物体移位了也能识别）。
        

#### 2. 核心计算公式 (必背)

假设输入图像宽为 $w_I$，卷积核宽为 $w_k$，步长为 $stride$，填充为 $padding$（课件中公式主要针对未填充或特定填充情况，我们以课件公式为准）。

- 输出特征图尺寸 (Feature Map Size)：
    $$w_f = \frac{w_I - w_k}{stride} + 1$$
    
    _(注意：如果除不尽，通常向下取整或根据框架定义，课件公式给出了标准形式)_ 。
    
- 参数量计算 (Parameter Count)：
    这是大题高频考点。假设输入通道数 $C_{in}$，输出通道数 $C_{out}$（即卷积核个数），卷积核尺寸 $K \times K$。
    
    $$参数量 = (K \times K \times C_{in} + 1) \times C_{out}$$
    
    - **解释**：每个卷积核有 $K \times K \times C_{in}$ 个权重，加上 1 个偏置 (bias)。
    - _注意_：池化层 (Pooling) 通常没有需要训练的参数。
        

#### 3. 关键层详解
- **卷积层 (Convolution)**：提取特征。多个卷积核提取多种特征（如有的提取横线，有的提取竖线）。
    
- **池化层 (Pooling)**：
    
    - **Max Pooling**：取窗口内最大值，提取最显著特征。
        
    - **Average Pooling**：取平均值。
        
    - _作用_：增大感受野，减少数据量。


# 第四部分：CNN 经典架构的演变

#### 1. LeNet-5 (开山鼻祖)

- **地位**：最早用于数字识别 (MNIST) 的 CNN 4。
    
- **结构**：卷积 -> 池化 -> 卷积 -> 池化 -> 全连接。
    
- **局限**：只能处理简单图像（如手写数字），层数较浅。
    

#### 2. AlexNet (历史突破)

- **地位**：2012 年 ImageNet 冠军，深度学习爆发的起点 5。
    
- **核心创新 (必背)** 6：
    
    - **ReLU**：首次大规模使用 ReLU 代替 Sigmoid，解决了梯度消失，加速收敛。
        
    - **Dropout**：随机忽略神经元，防止过拟合。
        
    - **Data Augmentation**：数据增广（旋转、裁剪）增加数据量。
        
    - **GPU**：使用了两块 GPU 训练（当时显存不够）。
        

#### 3. VGG (越深越好？)

- **核心理念**：**小卷积核，深网络** 7。
    
- **考点**：为什么用 2 个 $3 \times 3$ 卷积核代替 1 个 $5 \times 5$？8
    
    - **感受野相同**：都看 $5 \times 5$ 的区域。
        
    - **参数更少**：$2 \times (3 \times 3) = 18$ < $1 \times (5 \times 5) = 25$。
        
    - **非线性更强**：多了层激活函数，特征提取能力更强。
        

#### 4. GoogLeNet / Inception (更宽更好？)

- **核心创新**：**Inception 模块** 9。
    
- **痛点解决**：以前一层只能选一种卷积核（要么 $3 \times 3$，要么 $5 \times 5$）。Inception 说：“我全都要！”
    
    - **并行结构**：一层里同时用 $1 \times 1$、$3 \times 3$、$5 \times 5$ 卷积和池化，然后拼起来。
        
    - **$1 \times 1$ 卷积 (Bottleneck)**：用来**降维**，减少参数量（考点）10。
        

#### 5. ResNet (残差网络)

- **地位**：解决了深度网络的**退化问题**（层数越深，效果反而越差）。
    
- **核心结构**：**跳跃连接 (Shortcut/Skip Connection)** 。
    
    - 公式：$H(x) = F(x) + x$。让网络去学“残差”（差值），而不是直接学目标值。这让梯度能无损传导，网络可以做到非常深（如 152 层）。
        

#### 6. DenseNet (密集连接)

- **特点**：每一层都接收**前面所有层**作为输入。
    
- **区别**：ResNet 是**相加** (+)，DenseNet 是**拼接** (Concatenation) 。


# 第五部分：前沿架构——Attention 与 Transformer (最终章)

#### 1. Attention 机制：NLP 的里程碑

- 核心公式 (必考)：
    
    $$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
    
    - **Q (Query)**：查询向量。
        
    - **K (Key)**：键向量。
        
    - **V (Value)**：值向量。
        
    - **直观理解**：拿着 Query 去和所有的 Key 匹配（点积算相似度），根据相似度对 Value 进行加权求和。
        
- **多头注意力 (Multi-Head Attention)**：
    
    - 把 Q, K, V 分成多组（比如 8 组）并行计算，最后拼接起来。目的是让模型关注不同的特征子空间。
        

#### 2. Transformer 架构

- **结构**：由 **Encoder (编码器)** 和 **Decoder (解码器)** 组成。
    
    - _注意_：后续的视觉模型 ViT 只用了 Encoder。
        
- **位置编码 (Positional Encoding)**：因为 Transformer 并行处理序列，没有先后顺序概念，所以必须人为加上位置信息。
    

#### 3. Vision Transformer (ViT)：Transformer 进军视觉

- **如何处理图像？**
    
    1. **分块 (Patch)**：把二维图像切成一块块小补丁（比如 $16 \times 16$ 像素）。
        
    2. **拉平 (Flatten)**：把每个补丁拉成一维向量，这就变成了类似 NLP 里的“单词”序列 。
        
    3. **加上 Class Token**：增加一个可学习的类别标记，用于最后的分类输出。
        
- **ViT vs CNN (对比考点)**：
    
    - **CNN**：关注**局部**特征（归纳偏置强），小数据上表现好。
        
    - **ViT**：建立**全局**特征关联，在大规模数据集上训练后，效果超越 CNN。
        

#### 4. MAE (Masked Autoencoders)：ViT 的好搭档

- **核心策略**：**掩码重建 (Masked Reconstruction)**。
    
    - 把图片盖住一大半（比如随机遮挡 **75%** 的像素块），让模型去猜被盖住的是什么 。
        
- **意义**：这是一种**自监督学习**，不需要人工标注标签就能训练出很强的特征提取器 。



