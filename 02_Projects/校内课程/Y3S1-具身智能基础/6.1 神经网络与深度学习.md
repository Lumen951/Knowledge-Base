---
created: 2025-12-01
tags:
  - 笔记
---

# 第一部分 神经网络的“细胞”与基石

#### 1. 人工神经元模型 (M-P 模型)

深度学习模仿的是人脑的视觉机理，即从原始信号到低级抽象，再到高级抽象的迭代过程。

- **模型结构**：一个神经元接收来自其他神经元的输入 ($x_i$)，每个输入都有一个对应的**权重** ($w_i$)。
    
- 计算公式：
    
    $$y = f(\sum_{i=1}^{n} w_i x_i - \theta)$$
    
    这里 $\theta$ 是**阈值 (Threshold)**，$f(\cdot)$ 是**激活函数**。
    
- **直观理解**：神经元将输入加权求和，减去阈值，然后通过激活函数决定是否“兴奋”（输出）。
    

#### 2. 激活函数 (Activation Functions)

激活函数引入了**非线性**因素，使得神经网络能拟合复杂的函数。课件重点提到了两种：

- **Sigmoid 函数**：
    
    - 公式：$sigmoid(x) = \frac{1}{1+e^{-x}}$
        
    - 特点：将输出压缩到 (0, 1) 之间。
        
    - _缺点_（后续会讲）：容易导致梯度消失。
        
- **阶跃函数 (Step Function)**：$sgn(x)$，输出为 0 或 1，不可导，现在很少用。
    

#### 3. 感知机 (Perceptron) 与 XOR 问题

- **感知机能力**：罗森布拉特提出的感知机可以实现简单的逻辑运算，如**与 (AND)、或 (OR)、非 (NOT)**。
    
    - _原理_：通过调整权重 $w$ 和阈值 $\theta$，可以画出一条直线将两类模式分开（线性可分）。
        
- **致命缺陷 (XOR 问题)**：感知机无法解决**异或 (XOR)** 问题。
    
    - _原因_：异或问题是非线性可分的（在二维平面上找不到一条直线能把 (0,0)和(1,1) 分到一类，把 (0,1)和(1,0) 分到另一类）。
        
- **解决方案**：增加层数，引入**隐层 (Hidden Layer)**，构建多层前馈网络。
    

#### 4. 误差反向传播算法 (BP 算法)

这是多层网络训练的核心。

- **目标**：最小化损失函数（通常是均方误差 $E$）。
    
- **核心思想**：**梯度下降 (Gradient Descent)**。沿着误差梯度的负方向调整参数。
    
- **链式法则 (Chain Rule)**：BP 算法的数学本质。
    - 因为网络是层层嵌套的函数，计算第一层参数的梯度时，需要利用链式法则，将误差从输出层一层层向回传。
    - **核心公式**（不需死记，但要懂逻辑）：$\Delta w \propto -\eta \frac{\partial E}{\partial w}$，其中 $\eta$ 是学习率。


# 第二部分：深度学习的训练与优化

#### 1. 核心拦路虎：梯度消失 (Gradient Vanishing)

- **现象**：在深层网络中，误差反向传播回第一层时，梯度变得几乎为 0，导致前面的层根本没法更新参数，网络“死”了。
    
- **数学原因 (考点)**：
    
    - BP 算法基于**链式法则**（连乘。
        
    - 旧的激活函数 **Sigmoid** 的导数最大只有 **0.25**。
        
    - 多个小于 1 的数连乘（如 $0.25 \times 0.25 \times \dots$），结果会指数级衰减趋向于 0。
        

#### 2. 三大解决方案 (重点记忆)

为了解决梯度消失，深度学习引入了“三剑客”：

1. **改进激活函数：ReLU (修正线性单元)**
    
    - **公式**：$f(x) = \max(0, x)$ 。
        
    - **优势**：正区间的导数恒为 **1**。不管网络多深，只要激活了，梯度就能原样传回去，不会衰减。
        
    - _变体_：Leaky ReLU (解决负区间梯度为0的问题) 。
        
2. **批归一化 (Batch Normalization, BN)**
    
    - **原理**：在每一层的激活函数之前，强行把神经元的输入拉回到均值为 0、方差为 1 的标准正态分布。
        
    - **作用**：让输入值落在激活函数（哪怕是Sigmoid）的敏感区（非饱和区），避免梯度变小，同时允许使用更大的学习率。
        
3. **残差结构 (ResNet 的核心)**
    
    - **捷径连接 (Shortcut Connection)**：在层与层之间加一条“直连通路”。
        
    - **公式**：$y = F(x) + x$ 。
        
    - **原理**：求导时，$x$ 的导数是 1。这相当于给梯度开了一条“高速公路”，梯度可以无损地流向前面的层，不用完全经过复杂的非线性变换 $F(x)$ 。
        

#### 3. 另一拦路虎：过拟合 (Overfitting)

模型在训练集表现太好，泛化能力差。解决方法：

- **数据增广 (Data Augmentation)**：对图片进行旋转、裁剪、变色，人为制造更多数据。
    
- **提前停止 (Early Stopping)**：训练时监控验证集误差，一旦验证集误差不再下降（反而上升），就立刻停止训练。
    
- **Dropout**：训练时随机“关掉”一部分神经元（让它们不工作），迫使网络不依赖特定的神经元，增强鲁棒性。
    
- **正则化 (L1/L2)**：在损失函数里加一个惩罚项（如 $\lambda ||w||^2$），强迫权重 $w$ 尽可能小，让模型更简单平滑。
    

#### 4. 优化算法 (让梯度下降得更好)

- **SGD (随机梯度下降)**：每次只用这一个（或一批）样本算梯度，虽然有噪声，但能跳出局部极小值。
    
- **Momentum (动量)**：模拟物理动量，更新时加上“上一次的更新量”。像滚石下山，遇到小坑（局部极小）能冲过去。
    
- **Adam**：目前最常用的优化器。结合了 Momentum（动量）和 RMSProp（自适应学习率）的优点，收敛快且稳。


