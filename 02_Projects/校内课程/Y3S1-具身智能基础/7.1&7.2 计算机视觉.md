---
created: 2025-12-01
tags:
  - 笔记
---


# 第一部分复习：1. 概述

我们现在开始复习大纲中的第一部分：**1. 概述**。

#### 1.1 图像识别目标

理想目标：

让计算机能够像人类一样去理解图像的内容和含义 16。

实际目标：

让计算机能够将语义概念相似的图像划分到同一个类别中 17。例如，在计算机内部的向量空间中，网络通过优化，使得语义相似的图片（如都是“猫”）的特征向量在空间中的距离更近 18。

#### 1.2 图像识别挑战

图像识别面临的核心挑战是**语义鸿沟 (Semantic Gap)** 现象 19。

**语义鸿沟**指的是：图像的底层视觉特性（如颜色、纹理、形状等）和高层语义概念（如“猫”、“狗”、“建筑”）之间存在的差距 20。

- **挑战实例：** 有时，具有相似底层视觉特性（如相似的颜色、纹理）的图像，在人类看来却代表着完全不同的语义概念 21。计算机需要跨越这个鸿沟，才能真正理解图像。
    

#### 1.3 图像识别基本框架

**输入图像 → 特征提取 → 分类器 → 输出结果**



# 第二部分复习：2. 传统图像识别技术

#### 2.1 早期图像识别技术 (1990-2003)

- **核心思路：** **手工设计特征 + 简单分类器**。
    
    - 这个阶段主要依赖于人工经验和领域知识来设计和提取图像的特征。
        
- **关键技术点：**
    
    - **特征变换 (Feature Transformation)：** 对原始图像数据进行预处理，例如**中心化 (Centering)** 和**归一化 (Normalization)**，以消除光照和位置等因素的影响。
        
    - **流形学习/嵌入 (Manifold Learning/Embedding)：** 假设高维数据（图像像素）实际上位于一个低维的“流形”上。通过算法将高维特征映射到低维空间中，简化分类任务。
        
        - **代表算法：** PCA (主成分分析)、LDA (线性判别分析)。
            
- **局限性：** 缺乏鲁棒性。手工设计的特征往往只对特定的环境和数据集有效，泛化能力差。
    

#### 2.2 中期图像识别技术 (2003-2012)

- **核心思路：** **局部特征 + 视觉词袋模型 (Bag-of-Words, BoW)**。
    
    - 这一阶段开始关注图像中的局部、具有辨识性的区域特征，并借鉴了文本检索中的词袋模型思想。
        
- **关键技术点：**
    
    1. **局部特征提取：**
        
        - **目的：** 提取图像中具有尺度不变性、旋转不变性或仿射不变性的局部描述子。
            
        - **代表特征：**
            
            - **SIFT (Scale-Invariant Feature Transform)：** 尺度不变特征变换。
                
            - **SURF (Speeded Up Robust Features)：** 加速鲁棒特征。
                
            - **HOG (Histogram of Oriented Gradients)：** 方向梯度直方图（常用于行人检测）。
                
            - **GLOH (Gradient Location and Orientation Histogram)**。
                
    2. **视觉词袋模型 (BoW)：**
        
        - **步骤 I：视觉词典生成：**
            
            - 对从大量图像中提取出的局部特征进行聚类（如 K-Means）。
                
            - 每个聚类中心即为一个“**视觉词汇 (Visual Word)**”。所有视觉词汇构成“**视觉词典 (Visual Vocabulary)**”。
                
        - **步骤 II：图像表示：**
            
            - 将图像中的每个局部特征映射到它最近的视觉词汇。
                
            - 一幅图像就可以表示成一个**视觉词汇的直方图**，即词典中每个词汇出现的次数。
                
        - **步骤 III：分类：** 使用分类器（如 SVM）对直方图进行分类。
            
- **优点：** 相比早期技术，BoW 模型结合局部特征，显著提高了图像识别的鲁棒性和准确性。
    
- **局限性：** 特征提取和词典构建过程复杂且依赖参数，特征的表达能力仍有上限，无法捕获高层次的语义信息。

# 第三部分复习：3. 深度学习与图像识别

#### 3.1 深度学习发展历程



#### 3.2 为什么使用深度学习？

深度学习在图像识别中的优势在于：

1. **模仿人类视觉系统：** 深度神经网络（特别是卷积神经网络 CNN）的结构设计模仿了生物学上的视觉皮层，具备分层处理信息的特点。
    
2. **实现特征自动提取：**
    
    - 传统方法需要人工设计特征（如 SIFT, HOG），费时费力且效果有限。
        
    - **深度学习**通过**端到端 (End-to-End)** 的方式，自动从原始像素数据中学习和提取具有判别力的高层语义特征，完美解决了“语义鸿沟”问题。
        

#### 3.3 如何使用深度学习解决图像识别？

深度学习解决问题的过程可以归纳为**学习任务、建立模型、参数学习**三个核心步骤。

**1. 学习任务 (Learning Task): 寻找合适的函数 $f(\cdot)$**

- 目标是找到一个从输入图像 $X$ 到输出类别 $\hat{Y}$ 的映射函数 $f(X)$。
    
- 这个函数由神经网络的结构和参数（如权重 $w$、偏置 $b$）共同决定。
    
    $$\hat{Y} = f(X; \theta)$$
    
    其中 $\theta$ 代表所有待学习的参数。
    

**2. 建立模型 (Model Construction)**

- **网络结构 (Network Architecture):** 选择合适的深度网络类型（如 CNN, RNN）。确定层数、每层神经元数量等。
    
- **损失函数 (Loss Function $L$):** 用于衡量模型的预测值 $\hat{Y}$ 与真实值 $Y$ 之间的差异（即**损失**）。我们的目标是使总损失最小化。
    
    - **常用损失函数：**
        
        - **平方误差 (Squared Error)：** 常用于回归问题。
            
        - **交叉熵 (Cross-Entropy)：** 常用于分类问题，效果优于平方误差。
            
- **输出层 (Output Layer):**
    
    - **Softmax 函数：** 在图像分类任务中，通常将 Softmax 层放在网络的最后一层。它将网络的原始输出（logits）转化为**概率分布**，确保所有类别的概率和为 1，方便进行分类决策。
        

**3. 参数学习 (Parameter Learning): 最小化总损失**

- 目标是找到最优参数 $\theta^*$，使得所有训练样本上的总损失 $L$ 最小。
    
    $$\theta^* = \arg\min_{\theta} L(\theta)$$
    
- **关键算法：**
    
    - **梯度下降法 (Gradient Descent)：** 是一种优化算法，通过迭代地沿着损失函数梯度（导数）的**负方向**调整参数 $\theta$，使损失函数逐步减小，最终收敛到局部或全局最小值。
        
        - **调节规则：** 参数更新量与梯度成反比（乘以学习率 $\eta$）。
            
        - $w \leftarrow w - \eta \frac{\partial L}{\partial w}$
            
            - 当梯度为正时 ($\frac{\partial L}{\partial w} > 0$)，**减小**参数 $w$。
                
            - 当梯度为负时 ($\frac{\partial L}{\partial w} < 0$)，**增加**参数 $w$。
                
    - **反向传播算法 (Backpropagation, BP)：** 一种高效计算梯度的算法。它利用链式法则，从输出层开始，逐层向后计算每一层参数对总损失的贡献（即梯度），从而实现参数的更新。


# 第四部分复习：计算机视觉任务概述

计算机视觉领域不仅仅局限于简单的图像分类，还包含更复杂、更精细的视觉理解与生成任务。常见的典型任务包括：

1. **目标检测 (Object Detection)：**
    
    - **定义：** 在给定的图片中不仅要判断有什么物体（**分类**），还要精确找到物体所在的具体位置（**定位**），通常用矩形框（Bounding Box）标出 。
        
    - **核心：** “是什么？” + “在哪里？”
        
2. **图像分割 (Image Segmentation)：**
    
    - **定义：** 比目标检测更进一步，需要找到图像中的物体，并精确地勾勒出其所在的轮廓位置（通常是像素级别的分类）。
        
    - **核心：** 精确到像素的定位。
        
3. **图像标题生成 (Image Captioning)：**
    
    - **定义：** 理解图像内容，并为图像生成一句自然语言的描述文字 。
        
    - **核心：** 视觉与语言的结合（Vision + Language）。
        
4. **视觉内容生成 (Visual Content Generation)：**
    
    - **定义：** 根据输入条件（如文本描述、草图等）创作出一幅新的图像或一段新的视频 。
        
    - **核心：** 从无到有，或基于条件的创造。
        
5. **视频分类 & 动作识别 (Video Classification & Action Recognition)：**
    
    - **定义：** 将一段视频片段分类到指定的类别，或识别出视频中人物的动作类型 。
        
    - **核心：** 处理时间序列上的视觉信息。



# 第五部分复习：2.1 目标检测 (Object Detection)

#### 1. 任务定义与挑战

- **目标：** 在给定的图片中精确找到物体所在的**位置**（通常用矩形边框 Bounding Box 表示），并标注出物体的**类别** 。
    
- **核心难点：** 物体的尺寸变化范围很大，角度和姿态不确定，可能出现在图片的任何地方，且同一张图中可能包含多种类别的物体。
    

#### 2. 传统目标检测方法 (Traditional Methods)

在深度学习出现之前，目标检测主要依赖“手工特征 + 分类器”的模式。

- **基于滑动窗 (Sliding Window)：**
    
    - **原理：** 使用不同大小的窗口在图像上以一定步长滑动，对每个窗口内的图像块进行分类 。
        
    - **缺点：** 计算量巨大，难以实现实时检测 。
        
- **VJ Detector (Viola-Jones)：**
    
    - **特点：** 第一个实时的**人脸检测器** 。
        
    - **技术：** 使用 Haar-like 特征、积分图加速计算、Adaboost 级联分类器。
        
- **HOG (Histogram of Oriented Gradients)：**
    
    - **特点：** 在**行人检测**上效果显著。
        
    - **技术：** 提取图像的梯度方向直方图特征，结合 SVM 分类器。
        
- **DPM (Deformable Part Model)：**
    
    - **特点：** HOG 的进阶版，引入了**组件模型**。
        
    - **技术：** 包含根滤波器（整体）和组件滤波器（局部），允许物体部件发生一定的形变，通过空间位置约束来联合检测。
        

#### 3. 基于深度学习的目标检测 (Deep Learning Methods)

深度学习方法主要分为两大流派：**两阶段 (Two-stage)** 和 **一阶段 (One-stage)**。

A. 两阶段检测器 (Two-stage Detectors)

思路：先生成候选区域 (Region Proposal)，再进行分类和回归。精度高，但速度相对较慢。

1. **R-CNN (Regions with CNN features, 2014)** 
    
    - **流程：** 输入图像 -> 使用 Selective Search 提取约2000个候选框 -> **强制缩放 (Warp)** 到固定尺寸 -> 输入 CNN 提取特征 -> SVM 分类 + 边界框回归。
        
    - **缺点：** 计算量大（重复提取特征）、特征存储占用大、图像拉伸导致变形。
        
2. **SPP-Net (Spatial Pyramid Pooling, 2014)** 
    
    - **改进：** 引入**空间金字塔池化 (Spatial Pyramid Pooling)**。
        
    - **优势：** 一次性提取整图特征，不需要对每个候选区域重复计算 CNN，也不需要强制缩放图像。
        
3. **Fast R-CNN (2015)** 
    
    - **改进：** 引入 **RoI Pooling** 层，将不同大小的特征图映射为固定尺寸；使用多任务损失函数（分类+回归）统一训练。
        
4. **Faster R-CNN (2015)** 18
    
    - **改进：** 引入 **RPN (Region Proposal Network)** 代替 Selective Search。
        
    - **优势：** 实现了端到端 (End-to-End) 的检测，极大提升了速度，RPN 和检测网络共享卷积特征。
        
5. **FPN (Feature Pyramid Network, 2017)** 
    
    - **改进：** 构建特征金字塔，融合深层（强语义）和浅层（强位置信息）特征。
        
    - **优势：** 显著提升了**小目标**的检测能力。
        
6. **Mask R-CNN (2017)** 23
    
    - **改进：** 在 Faster R-CNN 基础上增加了 Mask 分支用于实例分割；提出 **RoI Align** 替代 RoI Pooling，解决了特征图与原图坐标不对应（Misalignment）的问题。
        

B. 一阶段检测器 (One-stage Detectors)

思路：直接在特征图上进行密集预测，一步到位。速度快，但早期精度略低。

1. **YOLO (You Only Look Once, 2016)** 
    
    - **原理：** 将图像划分为 $S \times S$ 的网格，每个网格直接预测边界框和类别概率。
        
    - **优势：** 速度极快，利用全局信息，背景误检率低。
        
2. **SSD (Single Shot MultiBox Detector, 2016)** 
    
    - **原理：** 在不同尺度的特征图上使用**预设框 (Default Boxes / Anchors)** 进行预测。
        
    - **优势：** 结合了 YOLO 的速度和 RPN 的 Anchor 思想，对多尺度物体检测效果较好。
        
3. **RetinaNet (2017)** 
    
    - **创新：** 提出了 **Focal Loss**。
        
    - **解决问题：** 解决了一阶段检测器中极端的**类别不平衡**问题（背景负样本远多于正样本），让模型更关注难以分类的样本。
        

#### 4. 跨模态与开放词汇检测 (New Trends)

- **开放词汇目标检测 (Open Vocabulary Object Detection)：** 旨在检测训练集中未见过的物体类别，通常利用视觉-语言模型实现。
    
- **CLIP (Contrastive Language-Image Pre-training)：** 通过对比学习将图像和文本嵌入到同一空间，支持 Zero-Shot 分类。
    
- **Grounding DINO：** 结合了 DINO 检测器和文本编码器，能根据自然语言描述（如“左边的狮子”）进行目标检测。

# 第六部分复习：2.2 图像分割 (Image Segmentation)

#### 1. 三种主要分割任务 (Definitions)

在开始模型复习前，先区分三个容易混淆的概念：

- **语义分割 (Semantic Segmentation)：** 对图像中每个像素进行分类（例如：所有属于“人”的像素涂成红色，所有属于“车”的像素涂成蓝色）。**不区分**同一类别的不同个体（比如两个人会连成一片）。
    
- **实例分割 (Instance Segmentation)：** 结合了目标检测和语义分割。不仅要区分像素类别，还要**区分不同的个体**（例如：张三的像素是红色，李四的像素是绿色）。
    
- **全景分割 (Panoptic Segmentation)：** 结合了语义分割和实例分割。既包含了可数的物体（如人、车），也包含了不可数的背景（如天空、草地）。
    

#### 2. 核心分割网络模型

我们将按照技术演进的逻辑来复习几个里程碑式的网络：

**A. 开山之作：FCN (Fully Convolutional Networks, 2015)**

- **核心思想：** 将传统分类网络（如 VGG）末端的全连接层（FC）替换为**卷积层**，从而输出空间特征图（Heatmap）而不是简单的分类分数 。
    
- **技术特点：** 使用**反卷积 (Deconvolution)** 进行上采样，将缩小的特征图恢复到原图大小，实现端到端的像素级预测 。
    
- **地位：** 深度学习在图像分割领域的里程碑 。
    

**B. 实时分割：SegNet (2017)**

- **应用场景：** 专注于**自动驾驶**道路场景，对**速度**要求极高（实时性）。
    
- **核心结构：** 经典的**编码器-解码器 (Encoder-Decoder)** 结构。
    
- **关键创新：** 在解码器上采样时，利用编码器阶段记录的**最大池化索引 (Max-pooling Indices)**。只在对应位置恢复数值，其他位置补零。这种方法不需要学习额外的上采样参数，大大加快了速度。
    

**C. 医学影像霸主：U-Net (2015)**

- **应用场景：** **医学影像分割**（对精度要求高，数据量通常较少）。
    
- **核心结构：** 也是 Encoder-Decoder，形状像字母 "U"。
    
- **关键创新：** 引入了 **跳跃连接 (Skip Connections)**。在解码器恢复分辨率时，将编码器对应层的特征图**拼接 (Concatenate)** 过来（Copy and Crop），融合了浅层特征，保留了更多细节。
    

**D. 扩大感受野：DeepLab 系列**

- **核心痛点：** 普通卷积和池化会降低分辨率，丢失细节。
    
- **关键创新：**
    
    - **扩张卷积/空洞卷积 (Dilated/Atrous Convolution)：** 在卷积核中“补零”，在不增加参数量和降低分辨率的情况下，指数级扩大**感受野**。
        
    - **ASPP (空洞空间金字塔池化)：** 并行使用不同扩张率的空洞卷积，提取多尺度特征。
        

**E. 实例分割代表：Mask R-CNN (2017)**

- **结构：** 在 Faster R-CNN 的基础上增加了一个**全卷积分支 (Mask Branch)** 用于预测分割掩码。
    
- **关键创新：** 提出了 **RoI Align** 技术，取代了 RoI Pooling。RoI Align 使用双线性插值解决了特征图与原图坐标不对应（Misalignment）的问题，这对像素级分割至关重要。
    

**F. 全景分割：Panoptic FPN (2019)**

- **结构：** 结合了 Mask R-CNN（负责实例分割分支）和 FCN（负责语义分割分支），并利用 FPN 提取强特征。
    

#### 3. 通用分割大模型 (General Segmentation)

- **SAM (Segment Anything Model)：**
    
    - **目标：** 分割任何图像中的任何物体（Segment Anything）。
        
    - **特点：** 支持**多模态提示 (Prompt)**，如点击一个点、画一个框或输入文本，模型就能自动分割出对应物体。
        
    - **结构：** 强大的图像编码器 + 提示编码器 + 掩码解码器。
        

---



# 第七部分复习：3. 图像生成任务

图像生成主要分为三大块内容：图像标题生成、视觉内容生成（如超分辨率、风格转换）和最新的视频生成。

#### 3.1 图像标题生成 (Image Captioning)

- **任务：** 为给定的图像自动生成一句准确、流畅的自然语言描述。
    
- **核心模型：** **Encoder-Decoder 结构**。
    
    - **Encoder（编码器）：** 使用 **Deep CNN**（如 VGG, ResNet）将输入的图像编码成一个具有语义信息的**特征向量**。
        
    - **Decoder（解码器）：** 使用 **RNN/LSTM** 或 **Transformer** 结构，将 Encoder 输出的特征向量作为输入，**逐词**生成描述图像的句子。
        
- **原理：** 实现了视觉 (Vision) 到语言 (Language) 的跨模态转换。
    

#### 3.2 视觉内容生成 (Visual Content Generation)

这一类任务旨在改变或提升图像的视觉质量或风格。

**A. 图像超分辨率 (Super Resolution)**

- **任务：** 将低分辨率 (Low Resolution, LR) 的图像重建为高分辨率 (High Resolution, HR) 的图像，以恢复图像细节。
    
- **关键技术：**
    
    - **SRGAN (Super-Resolution Generative Adversarial Network)：** 将 **GAN（生成对抗网络）**引入超分辨率领域。
        
        - **生成器 (Generator)：** 负责生成高分辨率图像。
            
        - **判别器 (Discriminator)：** 负责判断生成的高分辨率图像是真实的还是生成的。
            
    - **优势：** SRGAN 引入的感知损失 (Perceptual Loss) 使生成的图像在视觉上更加逼真，而不是仅仅追求像素上的低均方误差（MSE）。
        

**B. 图像风格转换 (Image Style Transfer)**

- **任务：** 将一幅图像的内容（如一张照片）和另一幅图像的风格（如一幅梵高画作）进行融合，生成一幅新的图像。
    
- **关键模型：**
    
    - **CycleGAN：**
        
        - **原理：** 解决**无监督 (Unpaired)** 风格转换问题（即没有成对的“照片-画作”训练数据）。
            
        - **创新：** 引入**循环一致性损失 (Cycle Consistency Loss)**。例如，将“马”转换为“斑马”，再将“斑马”转换回“马”，要求转换后的结果与原始图像尽可能接近。
            
    - **GcGAN (Geometry-Consistent Generative Adversarial Network)：**
        
        - **原理：** 在无监督风格转换的基础上，加入了**几何一致性 (Geometry-Consistent)** 约束。
            
        - **优势：** 在进行图像风格转换的同时，能更好地保持原始图像中物体的几何结构和姿态（如将“冬天的路”转换为“夏天的路”，路和树的形状不会扭曲）。
            

**C. 人工智能换脸 (Face Swap/DeepFake)**

- **技术：** 主要基于生成对抗网络（GAN）或自编码器（Auto-Encoder）技术。
    
- **应用：** 降低卡通化应用成本、利好电影工业（如换演员、重拍镜头）。
    
- **潜在风险：** 技术滥用，生成假图像/视频传播虚假信息，影响基于人脸识别的应用。
    

#### 3.3 视频生成 (Video Generation)

- **任务：** 根据文字描述（Prompt）或输入图像生成一段连贯、逼真的视频序列。
    
- **关键模型：**
    
    - **Sora (由 OpenAI 推出)：** 一个可以根据文本指令生成逼真且富有想象力的视频的**文本到视频生成模型**。
        
    - **核心技术：** 通常基于 **Diffusion Model（扩散模型）**和 **Transformer 架构**，能够理解并模拟复杂的物理世界和长距离时间依赖关系，生成高时空一致性的视频。



